{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0fcce-518a-4f15-93f2-cec9ef050161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb55f2-b3db-4664-8578-bdc257ed29d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "import srsly\n",
    "from rich import print\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from mgi.defaults import ROOT_PATH, PLOTS_PATH\n",
    "from mgi.data.sampled_datasets import load_sampled_datasets_metadata\n",
    "from dataclasses import asdict\n",
    "\n",
    "os.chdir(ROOT_PATH)\n",
    "\n",
    "from mgi.utils.config import load_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4fb59-a727-4e02-9aec-5c6acce0960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb4dcb-f60d-42f3-bbbe-be61ff44fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"paper\", style=\"whitegrid\", palette=\"colorblind\", font_scale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19f64c-739c-4107-87c3-be5c36b78b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = PLOTS_PATH / \"experiment_analysis\"\n",
    "plots_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3cb60-26d5-46fa-b34d-91b3ac92863e",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c2d1a-580b-4be7-9f00-c39c355fe1c6",
   "metadata": {},
   "source": [
    "### Configs loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce418d92-921c-49fd-9fd8-2f5168df417c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "for x in os.scandir(ROOT_PATH / \"experiments/configs/training/training_items\"):\n",
    "    [config_training_items] = srsly.read_yaml(x).values()\n",
    "    configs += [load_training_config(**item) for item in config_training_items]\n",
    "\n",
    "configs = [OmegaConf.to_container(c, resolve=True) for c in configs]\n",
    "configs_df = pd.DataFrame(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646db627-fb53-41bd-842c-99395d2a0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_df[\"run_name\"] = [\n",
    "    [f\"{x.experiment_name}_{s}\" for s in x.random_seed[: x.repeats]]\n",
    "    for x in configs_df.itertuples()\n",
    "]\n",
    "configs_df = configs_df.explode(\"run_name\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de8ee7-92d0-45ef-bcf8-868fa35db8d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea088cb9-3624-4738-8941-67f711335555",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\"data/experiments_results/results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902c335-f092-43d3-87b3-73fa7b2fbc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert not any(results_df.name.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f664c-11a3-46b6-844c-92cce993ce1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Not executed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b69a7d-00f8-47ed-a5b0-7113ac8d1581",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs_df[~configs_df.run_name.isin(results_df.name)].run_name.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eab8e8-76ea-49b6-8f54-94c53047ecc6",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9b3ae-4d2d-4dc6-b487-e33ebc6aa30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [results_df, pd.json_normalize(results_df.config), pd.json_normalize(results_df.summary)],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f39b4-9a31-4277-86d3-6f4402a42482",
   "metadata": {},
   "source": [
    "### Sampling params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc501b-925f-46de-9969-9970fd5fe087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling = pd.json_normalize(\n",
    "    df[\"config.ds_dataset\"]\n",
    "    .map(load_sampled_datasets_metadata())\n",
    "    .apply(lambda x: asdict(x) if pd.notna(x) else x)\n",
    ")\n",
    "sampling = sampling.drop(columns=\"name\")\n",
    "df = pd.concat([df, sampling], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337721d-9742-48fa-84ea-58d1147ba898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"is_full\"] = df[\"sampling_config.sampling\"].isna()\n",
    "df[\"is_combined\"] = df[\"config.gk_dataset\"] != \"???\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bde2f-dac3-44f8-8dad-e06a938da2d9",
   "metadata": {},
   "source": [
    "### Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef5eca-55ac-4206-a8e3-27b1428b2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\n",
    "    c.replace(\"config.loss\", \"loss_func\")\n",
    "    .removeprefix(\"testing.both.optimistic.\")\n",
    "    .removeprefix(\"config.\")\n",
    "    .removeprefix(\"sampling_config.\")\n",
    "    .replace(\"hits_at_\", \"Hits@\")\n",
    "    .replace(\"inverse_harmonic_mean_rank\", \"MRR\")\n",
    "    .replace(\"arithmetic_mean_rank\", \"MR\")\n",
    "    .replace(\"val/.both.optimistic.Hits@10\", \"val_Hits@10\")\n",
    "    for c in df.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fd5b5-8a56-4b79-b6b9-c1e30a4c8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(df.columns.get_loc(\"model\"))\n",
    "mask[np.argmax(df.columns.get_loc(\"model\"))] = True\n",
    "df = df.loc[:, ~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da47cd-0e0a-4ad9-af10-b7560190ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.ds_dataset.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c865da-c947-4220-9166-acf6ae180bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ds_dataset_parent\"] = df.ds_dataset.str.split(\"_\").apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f00b00-0d13-44a5-b903-41347ab403b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"crop_gk_n\"] = df[\"crop_gk_n\"].fillna(\"full\")\n",
    "df[\"graph_type\"] = df[\"is_combined\"].apply(lambda x: \"linked\" if x else \"single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7a8af-81bd-4a08-93af-caab4ad78253",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aef193-5798-44ae-a006-e747134e6c52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## w/o gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cde98a-88b2-482f-aeea-a4df74148634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS_DATASET_PARENTS = [\"WN18RR\", \"FB15K237\", \"WD50K\"]\n",
    "COLUMNS_TO_KEEP = [\"is_combined\", \"sampling\", \"p\"]\n",
    "\n",
    "X = \"ds_dataset\"\n",
    "HITS_10_METRIC = \"Hits@10\"\n",
    "MR_METRIC = \"MR\"\n",
    "MRR_METRIC = \"MRR\"\n",
    "HUE = \"combine_method\"\n",
    "SERACH_METRIC = \"val_Hits@10\"\n",
    "\n",
    "HITS_METRICS = (\n",
    "    \"Hits@1\",\n",
    "    \"Hits@3\",\n",
    "    \"Hits@10\",\n",
    ")\n",
    "\n",
    "SAMPLINGS = [\"triple\", \"node\", \"relation\"]\n",
    "METRICS_TO_PLOT = [HITS_10_METRIC, MR_METRIC, MRR_METRIC]\n",
    "# METRICS_TO_PLOT = [*HITS_METRICS, MR_METRIC, MRR_METRIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365dc98-0bff-4eea-af1a-c0f9eb877c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_df = df[~df.is_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21272c84-d024-4fea-ba97-eeb834d7aedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_results_df = (\n",
    "    standard_df.groupby([\"ds_dataset_parent\", X, *COLUMNS_TO_KEEP], dropna=False)[\n",
    "        [*HITS_METRICS, MR_METRIC, MRR_METRIC]\n",
    "    ]\n",
    "    .agg([\"mean\", \"std\", \"size\"])\n",
    "    .reset_index()\n",
    ")\n",
    "with pd.option_context(\"display.float_format\", \"{:.3f}\".format):\n",
    "    display(standard_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a36149-a621-488a-8740-95eb3f18f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_dataset_parent in DS_DATASET_PARENTS:\n",
    "    samplings = [\n",
    "        s\n",
    "        for s in SAMPLINGS\n",
    "        if s in df[df.ds_dataset_parent == ds_dataset_parent][\"sampling\"].unique()\n",
    "    ]\n",
    "    fig, axes = plt.subplots(len(METRICS_TO_PLOT), len(samplings), figsize=(len(samplings) * 3, 7))\n",
    "    for metric_, row_axes in zip(METRICS_TO_PLOT, axes):\n",
    "        for sampling, ax in zip(samplings, row_axes):\n",
    "            to_plot = df[~df.is_combined]\n",
    "            to_plot = to_plot[to_plot.ds_dataset_parent == ds_dataset_parent]\n",
    "            to_plot = to_plot[\n",
    "                (standard_df[\"sampling\"] == sampling) | (standard_df[\"sampling\"].isna())\n",
    "            ]\n",
    "            to_plot[\"sampling probability\"] = (to_plot.p.fillna(1.0) * 100).apply(\n",
    "                lambda x: f\"{int(x)}%\"\n",
    "            )\n",
    "            to_plot = to_plot.sort_values(by=[\"is_full\", X])\n",
    "            g = sns.barplot(to_plot, x=\"sampling probability\", y=metric_, ax=ax)\n",
    "            # ax.xaxis.set_tick_params(rotation=90)\n",
    "            ax.set_title(f\"sampling={sampling}\")\n",
    "    fig.suptitle(f\"{ds_dataset_parent} w/o general knowledge\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346d429-86c3-4612-96cf-117022f435e1",
   "metadata": {},
   "source": [
    "# Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47632f19-ef9b-400f-8c34-fd11a13f1671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cn_hparams = [\"alignment_k\", \"loss_func\", \"crop_gk_n\"]\n",
    "wn_hparams = [\"alignment_k\", \"loss_func\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6da928-e10d-4d74-b2f4-1e3c1d1bc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = df[df.is_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdfa39-ee17-4391-ba03-9b5d15d033bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = (\n",
    "    combined_df[combined_df[\"gk_dataset\"] == combined_df.ds_dataset_parent]\n",
    "    .groupby([\"ds_dataset_parent\", \"gk_dataset\", X, *wn_hparams, *COLUMNS_TO_KEEP])[\n",
    "        METRICS_TO_PLOT + [SERACH_METRIC]\n",
    "    ]\n",
    "    .agg([\"mean\", \"std\", \"size\"])\n",
    ")\n",
    "assert (grouped[HITS_10_METRIC][\"size\"] == 3).all()\n",
    "g = grouped.reset_index()\n",
    "results_per_ds_dataset = g.loc[g.groupby(X)[SERACH_METRIC].idxmax()[SERACH_METRIC][\"mean\"]]\n",
    "\n",
    "with pd.option_context(\"display.float_format\", \"{:.3f}\".format):\n",
    "    display(results_per_ds_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30047920-de68-44ab-8a25-abf859c09335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_max(row, col_1, col_2):\n",
    "    if row[(col_1, \"mean\")] > row[(col_2, \"mean\")]:\n",
    "        return [HIGHLIGHT_STYLE if col[0] == col_1 else \"\" for col in row.index]\n",
    "    elif row[(col_2, \"mean\")] > row[(col_1, \"mean\")]:\n",
    "        return [HIGHLIGHT_STYLE if col[0] == col_2 else \"\" for col in row.index]\n",
    "    else:\n",
    "        return [\"\" for _ in row.index]\n",
    "\n",
    "\n",
    "def bold_min(row, col_1, col_2):\n",
    "    if row[(col_1, \"mean\")] < row[(col_2, \"mean\")]:\n",
    "        return [HIGHLIGHT_STYLE if col[0] == col_1 else \"\" for col in row.index]\n",
    "    elif row[(col_2, \"mean\")] < row[(col_1, \"mean\")]:\n",
    "        return [HIGHLIGHT_STYLE if col[0] == col_2 else \"\" for col in row.index]\n",
    "    else:\n",
    "        return [\"\" for _ in row.index]\n",
    "\n",
    "\n",
    "def bold_max_2(row, compare_col_1, compare_col_2, highlight_col_1, highlight_col_2):\n",
    "    # print(row.index)\n",
    "    if row[(compare_col_1, \"mean\")] > row[(compare_col_2, \"mean\")]:\n",
    "        # print([\"background-color: grey\" if col == highlight_col_1 else \"\" for col in row.index])\n",
    "        return [HIGHLIGHT_STYLE if col == highlight_col_1 else \"\" for col in row.index]\n",
    "    elif row[(compare_col_2, \"mean\")] > row[(compare_col_1, \"mean\")]:\n",
    "        # print([\"background-color: grey\" if col == highlight_col_2 else \"\" for col in row.index])\n",
    "        return [HIGHLIGHT_STYLE if col == highlight_col_2 else \"\" for col in row.index]\n",
    "    else:\n",
    "        return [\"\" for _ in row.index]\n",
    "\n",
    "\n",
    "def bold_min_2(row, compare_col_1, compare_col_2, highlight_col_1, highlight_col_2):\n",
    "    if row[(compare_col_1, \"mean\")] < row[(compare_col_2, \"mean\")]:\n",
    "        return [HIGHLIGHT_STYLE if col == highlight_col_1 else \"\" for col in row.index]\n",
    "    elif row[(compare_col_2, \"mean\")] < row[(compare_col_1, \"mean\")]:\n",
    "        return [HIGHLIGHT_STYLE if col == highlight_col_2 else \"\" for col in row.index]\n",
    "    else:\n",
    "        return [\"\" for _ in row.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca951656-8748-425c-9a36-ece1ae6acd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table_from_styler(styler):\n",
    "    df = styler.data\n",
    "    latex_rows = []\n",
    "    column_names = df.columns.tolist()\n",
    "    for dataset, group in df.groupby(\"dataset\"):\n",
    "        separator_row = [\n",
    "            \"\\\\multicolumn{\" + str(len(column_names)) + \"}{c}{Dataset: \" + dataset + \"}\"\n",
    "        ]\n",
    "        latex_rows.extend(separator_row)\n",
    "        for _, row in group.iterrows():\n",
    "            data_row = [str(row[column]) for column in column_names]\n",
    "            latex_rows.append(\" & \".join(data_row))\n",
    "    latex_content = \"\\\\\\\\\\n\".join(latex_rows)\n",
    "    return latex_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92649d-fe49-40ec-afcf-cb3b9918a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_empty_rows_on_dataset_change(df):\n",
    "    empty_row = pd.DataFrame(columns=df.columns)\n",
    "    df_list = []\n",
    "    prev_dataset = None\n",
    "    for index, row in df.iterrows():\n",
    "        current_dataset = row[\"dataset\"]\n",
    "        if current_dataset.item() != prev_dataset:\n",
    "            if prev_dataset:\n",
    "                df_list.append(\n",
    "                    pd.Series(\n",
    "                        [\n",
    "                            df[df.dataset == prev_dataset][col].max()\n",
    "                            if col[1] == BOOST_COLUMN_NAME\n",
    "                            else \"\"\n",
    "                            for col in df.columns\n",
    "                        ],\n",
    "                        index=df.columns,\n",
    "                    )\n",
    "                )\n",
    "                df_list.append(\n",
    "                    pd.Series(\n",
    "                        [\n",
    "                            df[df.dataset == prev_dataset][col].mean()\n",
    "                            if col[1] == BOOST_COLUMN_NAME\n",
    "                            else \"\"\n",
    "                            for col in df.columns\n",
    "                        ],\n",
    "                        index=df.columns,\n",
    "                    )\n",
    "                )\n",
    "            df_list.append(pd.Series([current_dataset.item()] * len(df.columns), index=df.columns))\n",
    "        df_list.append(row)\n",
    "        prev_dataset = current_dataset.item()\n",
    "    df_list.append(\n",
    "        pd.Series(\n",
    "            [\n",
    "                df[df.dataset == prev_dataset][col].max() if col[1] == BOOST_COLUMN_NAME else \"\"\n",
    "                for col in df.columns\n",
    "            ],\n",
    "            index=df.columns,\n",
    "        )\n",
    "    )\n",
    "    df_list.append(\n",
    "        pd.Series(\n",
    "            [\n",
    "                df[df.dataset == prev_dataset][col].mean() if col[1] == BOOST_COLUMN_NAME else \"\"\n",
    "                for col in df.columns\n",
    "            ],\n",
    "            index=df.columns,\n",
    "        )\n",
    "    )\n",
    "    result_df = pd.DataFrame(df_list)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261c36b-4b31-48bf-af8a-2d66f500c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGHLIGHT_STYLE = \"font-weight: bold\"\n",
    "BOOST_COLUMN_NAME = \"boost (%)\"\n",
    "merged = pd.merge(\n",
    "    standard_results_df,\n",
    "    results_per_ds_dataset,\n",
    "    on=\"ds_dataset\",\n",
    "    suffixes=(\"_standard\", \"_combined\"),\n",
    ")\n",
    "\n",
    "METRICS_IN_TABLE = [\"Hits@1\", \"Hits@3\", \"Hits@10\", MR_METRIC, MRR_METRIC]\n",
    "METRICS_IN_TABLE = [\"Hits@10\", MR_METRIC, MRR_METRIC]\n",
    "\n",
    "for metric in METRICS_IN_TABLE:\n",
    "    decimal_places = 0 if metric == MR_METRIC else 3\n",
    "    merged[(metric, \"single\")] = (\n",
    "        merged[[(f\"{metric}_standard\", \"mean\"), (f\"{metric}_standard\", \"std\")]]\n",
    "    ).apply(lambda x: f\"{x[0]:.{decimal_places}f}±{x[1]:>3.{decimal_places}f}\", axis=1)\n",
    "    merged[(metric, \"combined\")] = (\n",
    "        merged[[(f\"{metric}_combined\", \"mean\"), (f\"{metric}_combined\", \"std\")]]\n",
    "    ).apply(lambda x: f\"{x[0]:.{decimal_places}f}±{x[1]:>3.{decimal_places}f}\", axis=1)\n",
    "    merged[(metric, BOOST_COLUMN_NAME)] = (\n",
    "        (merged[(f\"{metric}_combined\", \"mean\")] - merged[(f\"{metric}_standard\", \"mean\")])\n",
    "        / merged[(f\"{metric}_standard\", \"mean\")]\n",
    "    ) * 100  # .apply(lambda x: f\"{x:.1%}\")\n",
    "\n",
    "merged[(MR_METRIC, BOOST_COLUMN_NAME)] = -merged[(MR_METRIC, BOOST_COLUMN_NAME)]\n",
    "merged = merged.rename(\n",
    "    columns={\n",
    "        \"ds_dataset_parent_standard\": \"dataset\",\n",
    "        \"sampling_combined\": \"sampling\",\n",
    "        \"p_combined\": \"p\",\n",
    "    }\n",
    ")\n",
    "merged[\"dataset\"] = pd.Categorical(\n",
    "    merged[\"dataset\"], categories=[\"WN18RR\", \"FB15K237\", \"WD50K\"], ordered=True\n",
    ")\n",
    "merged[\"sampling\"] = pd.Categorical(\n",
    "    merged[\"sampling\"], categories=[\"triple\", \"node\", \"relation\"], ordered=True\n",
    ")\n",
    "merged = merged.sort_values([\"dataset\", \"sampling\"])\n",
    "merged_styled = add_empty_rows_on_dataset_change(merged)\n",
    "merged_styled = merged_styled[\n",
    "    [\n",
    "        # \"dataset\",\n",
    "        \"sampling\",\n",
    "        \"p\",\n",
    "        \"ds_dataset\",\n",
    "        \"alignment_k\",\n",
    "        \"loss_func\",\n",
    "        *METRICS_IN_TABLE,\n",
    "        *[\n",
    "            f\"{metric}_{graph_type}\"\n",
    "            for metric, graph_type in itertools.product(METRICS_TO_PLOT, [\"standard\", \"combined\"])\n",
    "        ],\n",
    "    ]\n",
    "].style.apply(\n",
    "    partial(bold_max, col_1=\"Hits@10_standard\", col_2=\"Hits@10_combined\"),\n",
    "    axis=1,\n",
    "    subset=[(\"Hits@10_standard\", \"mean\"), (\"Hits@10_combined\", \"mean\")],\n",
    ")\n",
    "merged_styled = merged_styled.apply(\n",
    "    partial(\n",
    "        bold_min_2,\n",
    "        compare_col_1=f\"{MR_METRIC}_standard\",\n",
    "        compare_col_2=f\"{MR_METRIC}_combined\",\n",
    "        highlight_col_1=(MR_METRIC, \"single\"),\n",
    "        highlight_col_2=(MR_METRIC, \"combined\"),\n",
    "    ),\n",
    "    axis=1,\n",
    "    subset=[\n",
    "        (f\"{MR_METRIC}_standard\", \"mean\"),\n",
    "        (f\"{MR_METRIC}_combined\", \"mean\"),\n",
    "        (MR_METRIC, \"single\"),\n",
    "        (MR_METRIC, \"combined\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "merged_styled = merged_styled.apply(\n",
    "    partial(bold_min, col_1=f\"{MR_METRIC}_standard\", col_2=f\"{MR_METRIC}_combined\"),\n",
    "    axis=1,\n",
    "    subset=[(f\"{MR_METRIC}_standard\", \"mean\"), (f\"{MR_METRIC}_combined\", \"mean\")],\n",
    ")\n",
    "\n",
    "for metric in METRICS_IN_TABLE:\n",
    "    if metric == MR_METRIC:\n",
    "        continue\n",
    "    merged_styled = merged_styled.apply(\n",
    "        partial(\n",
    "            bold_max_2,\n",
    "            compare_col_1=f\"{metric}_standard\",\n",
    "            compare_col_2=f\"{metric}_combined\",\n",
    "            highlight_col_1=(metric, \"single\"),\n",
    "            highlight_col_2=(metric, \"combined\"),\n",
    "        ),\n",
    "        axis=1,\n",
    "        subset=[\n",
    "            (f\"{metric}_standard\", \"mean\"),\n",
    "            (f\"{metric}_combined\", \"mean\"),\n",
    "            (metric, \"single\"),\n",
    "            (metric, \"combined\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "merged_styled = merged_styled.hide(\n",
    "    [(\"ds_dataset\", \"\"), (\"alignment_k\", \"\"), (\"loss_func\", \"\")]\n",
    "    + list(\n",
    "        itertools.product(\n",
    "            [\n",
    "                f\"{metric}_{graph_type}\"\n",
    "                for metric, graph_type in itertools.product(\n",
    "                    METRICS_TO_PLOT, [\"standard\", \"combined\"]\n",
    "                )\n",
    "            ],\n",
    "            [\"mean\", \"std\", \"size\"],\n",
    "        )\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "merged_styled = merged_styled.format(precision=1)\n",
    "\n",
    "# with pd.option_context(\"display.float_format\", \"{:.3f}\".format):\n",
    "display(merged_styled)\n",
    "print(\n",
    "    merged_styled.format_index(axis=1, formatter=\"${}$\".format)\n",
    "    .hide(axis=0)\n",
    "    .to_latex(convert_css=True)\n",
    "    .replace(\"%\", \"\\%\")\n",
    "    .replace(\"±\", \"\\pm\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a22fe3-97c4-4c88-9a69-59f8883daea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = merged.copy()\n",
    "\n",
    "for metric_ in METRICS_TO_PLOT:\n",
    "    to_plot[f\"{metric_}\\n{BOOST_COLUMN_NAME}\"] = to_plot[(metric_, BOOST_COLUMN_NAME)]\n",
    "\n",
    "samplings = SAMPLINGS\n",
    "fig, axes = plt.subplots(\n",
    "    len(METRICS_TO_PLOT),\n",
    "    len(samplings),\n",
    "    figsize=(len(samplings) * 1.65, 5),\n",
    "    sharex=True,\n",
    "    sharey=\"row\",\n",
    ")\n",
    "for i, (metric_, axes_row) in enumerate(zip(METRICS_TO_PLOT, axes)):\n",
    "    for sampling, ax in zip(samplings, axes_row):\n",
    "        to_plot_ax = to_plot[(to_plot[\"sampling\"] == sampling)]\n",
    "        g = sns.lineplot(\n",
    "            to_plot_ax.sort_values(X),\n",
    "            x=\"p\",\n",
    "            y=f\"{metric_}\\n{BOOST_COLUMN_NAME}\",\n",
    "            hue=\"dataset\",\n",
    "            ax=ax,\n",
    "            marker=\"o\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"sampling={sampling}\")\n",
    "        ax.get_legend().remove()\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, ncol=3, loc=\"upper center\")\n",
    "fig.suptitle(\"\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(plots_path / \"boost.png\", format=\"png\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae796ed9-bf71-4c5e-96ab-a0416c0db7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = df.merge(\n",
    "    results_per_ds_dataset[\n",
    "        [\n",
    "            \"ds_dataset_parent\",\n",
    "            \"gk_dataset\",\n",
    "            \"ds_dataset\",\n",
    "            \"alignment_k\",\n",
    "            \"loss_func\",\n",
    "            \"is_combined\",\n",
    "            \"sampling\",\n",
    "        ]\n",
    "    ].droplevel(1, axis=1),\n",
    "    on=[\n",
    "        \"ds_dataset_parent\",\n",
    "        \"gk_dataset\",\n",
    "        \"ds_dataset\",\n",
    "        \"alignment_k\",\n",
    "        \"loss_func\",\n",
    "        \"is_combined\",\n",
    "        \"sampling\",\n",
    "    ],\n",
    ")\n",
    "to_plot = pd.concat([df[~df.is_combined], to_plot])\n",
    "for ds_dataset_parent in DS_DATASET_PARENTS:\n",
    "    samplings = [\n",
    "        s\n",
    "        for s in SAMPLINGS\n",
    "        if s in to_plot[to_plot.ds_dataset_parent == ds_dataset_parent][\"sampling\"].unique()\n",
    "    ]\n",
    "    if ds_dataset_parent != \"FB15K237\":\n",
    "        figsize = (len(samplings) * 1.7, 5)\n",
    "    else:\n",
    "        figsize = (3.75, 5)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        len(METRICS_TO_PLOT),\n",
    "        len(samplings),\n",
    "        figsize=figsize,\n",
    "        # figsize=(5.1, 5),\n",
    "        sharex=True,\n",
    "        sharey=\"row\",\n",
    "    )\n",
    "    for i, (metric_, axes_row) in enumerate(zip(METRICS_TO_PLOT, axes)):\n",
    "        for sampling, ax in zip(samplings, axes_row):\n",
    "            to_plot_ax = to_plot[\n",
    "                (to_plot[\"sampling\"] == sampling)\n",
    "                & (to_plot[\"ds_dataset_parent\"] == ds_dataset_parent)\n",
    "            ]\n",
    "\n",
    "            g = sns.lineplot(\n",
    "                to_plot_ax.sort_values(X),\n",
    "                x=\"p\",\n",
    "                y=metric_,\n",
    "                ax=ax,\n",
    "                hue=\"graph_type\",\n",
    "                marker=\"o\",\n",
    "                linestyle=\"--\",\n",
    "                errorbar=\"sd\",\n",
    "            )\n",
    "            if sampling != \"relation\":\n",
    "                y_line = standard_results_df[standard_results_df.ds_dataset == ds_dataset_parent][\n",
    "                    (metric_, \"mean\")\n",
    "                ].item()\n",
    "                ax.axhline(\n",
    "                    y=y_line,\n",
    "                    color=sns.color_palette()[2],\n",
    "                    linewidth=1,\n",
    "                    label=\"original\",\n",
    "                )\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"sampling={sampling}\")\n",
    "            ax.get_legend().remove()\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        [handles[idx] for idx in [0, 1, 2]],\n",
    "        [labels[idx] for idx in [0, 1, 2]],\n",
    "        ncol=3,\n",
    "        loc=\"upper center\",\n",
    "    )\n",
    "    fig.suptitle(\"\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\n",
    "        plots_path / f\"results_{ds_dataset_parent}.png\", format=\"png\", dpi=600, bbox_inches=\"tight\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e60fa-daef-42a8-b83f-7a46157b63f6",
   "metadata": {},
   "source": [
    "## Realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49eba4-74f3-4387-812d-8e5852367d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_max_3(row, compare_cols, highlight_cols):\n",
    "    max_index = np.argmax([row[(c, \"mean\")].item() for c in compare_cols])\n",
    "    return [HIGHLIGHT_STYLE if col == highlight_cols[max_index] else \"\" for col in row.index]\n",
    "\n",
    "\n",
    "def bold_min_3(row, compare_cols, highlight_cols):\n",
    "    min_index = np.argmin([row[(c, \"mean\")].item() for c in compare_cols])\n",
    "    return [HIGHLIGHT_STYLE if col == highlight_cols[min_index] else \"\" for col in row.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a67b2d-effe-4240-bd9e-d5a9b189c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (\n",
    "    combined_df[\n",
    "        (combined_df[\"gk_dataset\"] != combined_df[\"ds_dataset_parent\"])\n",
    "        & (combined_df[\"ds_dataset\"] != combined_df[\"ds_dataset_parent\"])\n",
    "    ]\n",
    "    .groupby([\"gk_dataset\", X, *cn_hparams, \"is_combined\"])[METRICS_TO_PLOT + [SERACH_METRIC]]\n",
    "    .agg([\"mean\", \"std\", \"size\"])\n",
    ")\n",
    "\n",
    "g = grouped.reset_index()\n",
    "results_per_ds_dataset_cn = g.loc[\n",
    "    g.groupby([\"gk_dataset\", X])[SERACH_METRIC].idxmax()[SERACH_METRIC][\"mean\"]\n",
    "]\n",
    "\n",
    "merged = pd.merge(\n",
    "    standard_results_df,\n",
    "    results_per_ds_dataset_cn,\n",
    "    on=\"ds_dataset\",\n",
    "    suffixes=(\"_standard\", \"_combined\"),\n",
    ")\n",
    "\n",
    "for d in merged.ds_dataset_parent.unique():\n",
    "    gks = list(merged[(merged.ds_dataset_parent == d)].gk_dataset.unique())\n",
    "    print(d)\n",
    "    assert len(gks) == 2\n",
    "    merged_inner = pd.merge(\n",
    "        merged[(merged.ds_dataset_parent == d) & (merged.gk_dataset == gks[0])],\n",
    "        merged[(merged.ds_dataset_parent == d) & (merged.gk_dataset == gks[1])],\n",
    "        on=[\"ds_dataset_parent\", \"sampling\", \"p\"],\n",
    "        suffixes=(f\"_{gks[0]}\", f\"_{gks[1]}\"),\n",
    "    )\n",
    "    for metric in METRICS_IN_TABLE:\n",
    "        assert (\n",
    "            (\n",
    "                merged_inner[f\"{metric}_standard_{gks[0]}\"]\n",
    "                == merged_inner[f\"{metric}_standard_{gks[1]}\"]\n",
    "            )\n",
    "            .all()\n",
    "            .all()\n",
    "        )\n",
    "        merged_inner = merged_inner.rename(\n",
    "            columns={f\"{metric}_standard_{gks[0]}\": f\"{metric}_standard\"}\n",
    "        )\n",
    "        merged_inner.drop(columns=[f\"{metric}_standard_{gks[1]}\"])\n",
    "    merged_inner[\"\"] = \"\"\n",
    "    merged_inner = merged_inner.set_index(\"\", append=True).unstack(\"\")\n",
    "    for metric in METRICS_IN_TABLE:\n",
    "        decimal_places = 0 if metric == MR_METRIC else 3\n",
    "        merged_inner[(metric, \"single\", \"\")] = (\n",
    "            merged_inner[[(f\"{metric}_standard\", \"mean\", \"\"), (f\"{metric}_standard\", \"std\", \"\")]]\n",
    "        ).apply(\n",
    "            lambda x: f\"{x[0]:.{decimal_places}f}±{x[1]:>3.{decimal_places}f}\"\n",
    "            if pd.notna(x[1])\n",
    "            else f\"{x[0]:.{decimal_places}f}\",\n",
    "            axis=1,\n",
    "        )\n",
    "        merged_inner[(metric, f\"combined\", f\"{gks[0]}\")] = (\n",
    "            merged_inner[\n",
    "                [\n",
    "                    (f\"{metric}_combined_{gks[0]}\", \"mean\", \"\"),\n",
    "                    (f\"{metric}_combined_{gks[0]}\", \"std\", \"\"),\n",
    "                ]\n",
    "            ]\n",
    "        ).apply(\n",
    "            lambda x: f\"{x[0]:.{decimal_places}f}±{x[1]:>3.{decimal_places}f}\"\n",
    "            if pd.notna(x[1])\n",
    "            else f\"{x[0]:.{decimal_places}f}\",\n",
    "            axis=1,\n",
    "        )\n",
    "        merged_inner[(metric, f\"combined\", f\"{gks[1]}\")] = (\n",
    "            merged_inner[\n",
    "                [\n",
    "                    (f\"{metric}_combined_{gks[1]}\", \"mean\", \"\"),\n",
    "                    (f\"{metric}_combined_{gks[1]}\", \"std\", \"\"),\n",
    "                ]\n",
    "            ]\n",
    "        ).apply(\n",
    "            lambda x: f\"{x[0]:.{decimal_places}f}±{x[1]:>3.{decimal_places}f}\"\n",
    "            if pd.notna(x[1])\n",
    "            else f\"{x[0]:.{decimal_places}f}\",\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        if metric == MR_METRIC:\n",
    "            merged_inner[(metric, BOOST_COLUMN_NAME, \"\")] = (\n",
    "                (\n",
    "                    merged_inner[\n",
    "                        [\n",
    "                            (f\"{metric}_combined_{gks[0]}\", \"mean\", \"\"),\n",
    "                            (f\"{metric}_combined_{gks[1]}\", \"mean\", \"\"),\n",
    "                        ]\n",
    "                    ].min(1)\n",
    "                    - merged_inner[(f\"{metric}_standard\", \"mean\", \"\")]\n",
    "                )\n",
    "                / merged_inner[(f\"{metric}_standard\", \"mean\", \"\")]\n",
    "            ) * 100\n",
    "        else:\n",
    "            merged_inner[(metric, BOOST_COLUMN_NAME, \"\")] = (\n",
    "                (\n",
    "                    merged_inner[\n",
    "                        [\n",
    "                            (f\"{metric}_combined_{gks[0]}\", \"mean\", \"\"),\n",
    "                            (f\"{metric}_combined_{gks[1]}\", \"mean\", \"\"),\n",
    "                        ]\n",
    "                    ].max(1)\n",
    "                    - merged_inner[(f\"{metric}_standard\", \"mean\", \"\")]\n",
    "                )\n",
    "                / merged_inner[(f\"{metric}_standard\", \"mean\", \"\")]\n",
    "            ) * 100\n",
    "\n",
    "    merged_inner[(MR_METRIC, BOOST_COLUMN_NAME, \"\")] = -merged_inner[\n",
    "        (MR_METRIC, BOOST_COLUMN_NAME, \"\")\n",
    "    ]\n",
    "    merged_inner = merged_inner.rename(\n",
    "        columns={\n",
    "            \"ds_dataset_parent\": \"dataset\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    merged_inner[\"dataset\"] = pd.Categorical(\n",
    "        merged_inner[\"dataset\"], categories=[\"WN18RR\", \"FB15K237\", \"WD50K\"], ordered=True\n",
    "    )\n",
    "    merged_inner[\"sampling\"] = pd.Categorical(\n",
    "        merged_inner[\"sampling\"], categories=[\"triple\", \"node\", \"relation\"], ordered=True\n",
    "    )\n",
    "    merged_inner = merged_inner.sort_values([\"dataset\"])\n",
    "    merged_inner = add_empty_rows_on_dataset_change(merged_inner)\n",
    "    merged_styled = merged_inner[\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"sampling\",\n",
    "            \"p\",\n",
    "            *METRICS_IN_TABLE,\n",
    "            *[f\"{metric}_standard\" for metric in METRICS_TO_PLOT],\n",
    "            *[f\"{metric}_combined_{gk}\" for metric, gk in itertools.product(METRICS_TO_PLOT, gks)],\n",
    "        ]\n",
    "    ].style\n",
    "\n",
    "    merged_styled = merged_styled.apply(\n",
    "        partial(\n",
    "            bold_min_3,\n",
    "            compare_cols=[f\"{MR_METRIC}_standard\", *[f\"{MR_METRIC}_combined_{gk}\" for gk in gks]],\n",
    "            highlight_cols=[\n",
    "                (MR_METRIC, \"single\", \"\"),\n",
    "                *[(MR_METRIC, \"combined\", gk) for gk in gks],\n",
    "            ],\n",
    "        ),\n",
    "        axis=1,\n",
    "        subset=[\n",
    "            (f\"{MR_METRIC}_standard\", \"mean\", \"\"),\n",
    "            *[(f\"{MR_METRIC}_combined_{gk}\", \"mean\", \"\") for gk in gks],\n",
    "            (MR_METRIC, \"single\", \"\"),\n",
    "            *[(MR_METRIC, \"combined\", gk) for gk in gks],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for metric in METRICS_IN_TABLE:\n",
    "        if metric == MR_METRIC:\n",
    "            continue\n",
    "        merged_styled = merged_styled.apply(\n",
    "            partial(\n",
    "                bold_max_3,\n",
    "                compare_cols=[f\"{metric}_standard\", *[f\"{metric}_combined_{gk}\" for gk in gks]],\n",
    "                highlight_cols=[(metric, \"single\", \"\"), *[(metric, \"combined\", gk) for gk in gks]],\n",
    "            ),\n",
    "            axis=1,\n",
    "            subset=[\n",
    "                (f\"{metric}_standard\", \"mean\", \"\"),\n",
    "                *[(f\"{metric}_combined_{gk}\", \"mean\", \"\") for gk in gks],\n",
    "                (metric, \"single\", \"\"),\n",
    "                *[(metric, \"combined\", gk) for gk in gks],\n",
    "            ],\n",
    "        )\n",
    "    merged_styled = merged_styled.hide(\n",
    "        [(\"sampling\", \"\", \"\")]\n",
    "        + list(\n",
    "            itertools.product(\n",
    "                [f\"{metric}_standard\" for metric in METRICS_TO_PLOT], [\"mean\", \"std\", \"size\"], [\"\"]\n",
    "            )\n",
    "        )\n",
    "        + list(\n",
    "            itertools.product(\n",
    "                [\n",
    "                    f\"{metric}_combined_{gk}\"\n",
    "                    for metric, gk in itertools.product(METRICS_TO_PLOT, gks)\n",
    "                ],\n",
    "                [\"mean\", \"std\", \"size\"],\n",
    "                [\"\"],\n",
    "            )\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    merged_styled = merged_styled.format(precision=1)\n",
    "\n",
    "    with pd.option_context(\"display.float_format\", \"{:.1f}\".format):\n",
    "        display(merged_styled)\n",
    "        print(\n",
    "            merged_styled.format_index(axis=1, formatter=\"${}$\".format)\n",
    "            .hide(axis=0)\n",
    "            .to_latex(convert_css=True)\n",
    "            .replace(\"%\", \"\\%\")\n",
    "            .replace(\"±\", \"\\pm\")\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "22-multiple-graph-inference",
   "language": "python",
   "name": "22-multiple-graph-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
